---
title: "Transformers - Intro"
slug: "11_ai/01_applied_gen_ai/01_models/03_transformers"
stack: "GenAI"
date: "2025-06-03T07:26:45.889Z"
draft: false
---

<style>
o { color: #f49735 }
v { color: #b36be2 }
g { color: #70bf41 }
b { color: #62aefa }
</style>

> Limitations of Traditional Model

### ğŸ§  RNNs, LSTMs, and GRUs â€” The Old Champions

![Traditional Architecture - RNN](../../../../../src/images/11_ai/01_agen_ai/agi-13.png)

ğŸ§  **Introduction to Sequence Models**

- Designed for tasks where **order matters** â€” like speech, language, and time series.
- Dominated early breakthroughs in **language modeling, machine translation, and speech recognition**.

â“**How They Work** `Sequential Dynamics`

- **R**ecurrent **N**eural **N**etworks `RNN`s process input one time-step at a time.
- At each step, they pass a hidden state forward to carry information.
- Memory is encoded within these hidden states, enabling context retention

ğŸ§  **Variants with Memory Booster**

| Model | Core Feature           | Strengths                       | Weaknesses                |
| ----- | ---------------------- | ------------------------------- | ------------------------- |
| RNN   | Basic recurrence       | Simple and interpretable        | Poor long-term memory     |
| LSTM  | Long Short-Term Memory | Remembers over long distances   | Complex architecture      |
| GRU   | Gated Recurrent Unit   | Lightweight alternative to LSTM | Less expressive than LSTM |

â“ **Why They Fell Behind**

- _Information Bottleneck_ means **struggle with long sequences**
- Sequential computation means **no parallelism** â€” slows down training. ğŸŒ
- As sequences grow, **vanishing gradients** and **memory decay** kick in.ğŸŒˆ
- No Scalibility means **unable to extend to massive datasets**
- Workarounds like:
  - Memory cells `LSTM`
  - Gating mechanisms `GRU`
  - Selective attention  
    ...helped, but couldnâ€™t fully match the scalability needs of modern NLP.

ğŸ§  **Legacy & Transition**

- RNN-family models are still relevant in niche use cases or where simplicity matters.
- The rise of **Transformers** introduced parallelism and better long-range modeling.
- However, understanding **RNNs is essential** for grasping the evolution of sequence modeling.

> The Transformer Era â€” Parallelism Over Recurrence

### âœ¨ Attention Mechanisms â€” A Breakthrough

- Instead of just relying on previous words, **attention** lets the model â€œlook aroundâ€ at all words in the sequence, regardless of distance.
- Think of it like reading a whole paragraph at once, instead of word-by-word.

### ğŸš€ Transformers â€” The Game Changer

- No recurrence, no step-by-step processing.
- Fully powered by attention, so they learn relationships across the whole input instantly.
- They can train super fast and in parallel â€” reaching cutting-edge translation performance in just hours.

![Essence](../../../../../src/images/11_ai/01_agen_ai/agi-19a.png)
![Essence](../../../../../src/images/11_ai/01_agen_ai/agi-19b.png)

