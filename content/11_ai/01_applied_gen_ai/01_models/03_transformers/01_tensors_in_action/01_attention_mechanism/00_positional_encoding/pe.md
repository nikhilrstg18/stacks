---
title: "Positional Encoding"
slug: "11_ai/01_applied_gen_ai/01_models/03_transformers/01_tensors_in_action/01_attention_mechanism/00_positional_encoding"
stack: "GenAI"
date: "2025-10-18T07:26:45.889Z"
draft: false
---

> Transformers add extra vectors to each word to tell the model its spot in the sentence — so it doesn't lose the meaning behind word order.

### ❓ **Why Word Order Matters**

Transformers don’t process words in sequence like `RNN`s — they handle all words in parallel. But without sequence info, they wouldn't know what came first, second, or last

### ❓ **How Transformers Handle Word Position**

- They add a special `position vector` to each word's embedding.
- This vector carries information like: “This word is in position #3.”
- These `position vector`s follow a **pattern the model can learn** — like sine and cosine curves or learned embeddings.

![To give the model a sense of the order of the words, we add positional encoding vectors -- the values of which follow a specific pattern.](../../../../../../../../src/images/11_ai/01_agen_ai/agi-18.png)

If we assumed the embedding has a dimensionality of 4, the actual positional encodings would look like this:

![A real example of positional encoding with a toy embedding size of 4](../../../../../../../../src/images/11_ai/01_agen_ai/agi-18a.png)

The positional encoding shown above is from the Tensor2Tensor implementation of the Transformer. The method shown in the paper is slightly different in that it doesn’t directly concatenate, but interweaves the two signals. The following figure shows what that looks like. [Here’s the code to generate it](https://github.com/jalammar/jalammar.github.io/blob/master/notebookes/transformer/transformer_positional_encoding_graph.ipynb):

![A real example of positional encoding for 20 words (rows) with an embedding size of 512 (columns). You can see that it appears split in half down the center. That's because the values of the left half are generated by one function (which uses sine), and the right half is generated by another function (which uses cosine). They're then concatenated to form each of the positional encoding vectors.](../../../../../../../../src/images/11_ai/01_agen_ai/agi-18c.png)

### ❓ **Why This Works**

When combined with regular word embeddings:

- It gives the model a sense of **where each word is in the sentence**.
- Helps the attention layers **understand word order and relationships** — like how “because” relates to what came before.

<br/>
<br/>
<br/>
<br/>

---

For a visual deep dive, see

- _Alammar, J (2018). **The Illustrated Transformer** [Blog post]. Retrieved from https://jalammar.github.io/illustrated-transformer/_
