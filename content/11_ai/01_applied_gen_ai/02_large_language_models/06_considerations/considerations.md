---
title: "Considerations - `LLM`"
slug: "11_ai/01_applied_gen_ai/02_large_language_models/06_considerations"
stack: "GenAI"
date: "2025-06-03T07:26:45.889Z"
draft: false
---

![There are two types of considerations for choosing an `LLM`](../../../../../src/images/11_ai/01_agen_ai/agi-20j.png)

**The critical considerations for choosing an `LLM` are:**

1. Licensing and commercial use
2. Practical factors for inference speed and precision
3. The impact of context length and model size
4. Task-specific vs. general purpose
5. Testing and evaluation
6. Deployment cost considerations

**The technical considerations for choosing an `LLM` are:**

1. Data security and privacy
2. Model inference monitoring
3. Scalability and performance
4. Version control and updating
5. APIs and integration security

### Future Implications of `LLM`s

Large Language Models `LLM`s like GPT-4, LLaMA 2, Mistral, and future multi-modal AI systems will have both positive and negative implications

**Positive Implications**

- Advancements in Productivity & Automation
- Enhancements in Healthcare & Drug Discovery
- Education & Personalized Learning
- Multi-modal AI & Human-AI Collaboration

**Negative Implications**

- Job Displacement & Economic Impact
- Bias, Misinformation, & Deepfakes
- Security & Privacy Risks
- Dependency on AI & Ethical Concerns

### Case Study-1

> AI Hallucinations and Misinformation in `LLM`s

**Case Example: Google Bard’s Incorrect Response**

In 2023, Google Bard (now Gemini) provided incorrect scientific information in a live demo. The model falsely claimed that James Webb Space Telescope took the first-ever exoplanet image, which was incorrect.
Ethical Concerns

- `Overconfidence in AI responses` –`LLM`s generate plausible but incorrect information.
- `Trust & Public Perception` –AI systems deployed in critical fields must ensure factual accuracy.
- `Misinformation Impact` –In domains like medicine, law, and finance, AI errors can lead to wrong decisions.

### Case Study-2

> Bias in AI Decision-Making

**Case Example: Amazon’s AI Hiring Bias (2018)**

Amazon developed an AI-driven recruitment tool to screen job applicants, but it was found to discriminate against female candidates. The model favored male applicants because it was trained on past hiring data, which was biased.
Ethical Concerns

- `Historical Bias Replication` –AI models inherit biases from historical data.
- `Lack of Fairness in AI Decision-Making` –AI-powered hiring tools must ensure equitable candidate evaluation.
- `AI as a Gatekeeper` –How much should AI be trusted in making human-impactful decisions?

### Case Study-3

> AI-Powered Deepfakes & Digital Manipulation

**Case Example: Political Deepfake Videos in Elections**

In 2024, AI-generated deepfake videos of political figures surfaced online, spreading misinformation. Deepfake technologies powered by `LLM`s + image models (DALL·E, Stable Diffusion) were used to alter speeches, forge identities, and manipulate voters.
Ethical Concerns

- `Democracy & Misinformation` –AI-generated deepfakes can alter public perception during elections.
- `Fake Identity & Fraud` –AI can create realistic fake personas for scamming and social engineering.
- `Regulation Challenges` –How do we differentiate real vs. AI-generated content?
