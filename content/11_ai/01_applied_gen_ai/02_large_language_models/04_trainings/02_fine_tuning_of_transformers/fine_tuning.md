---
title: "Fine-Tuning a Transformer"
slug: "11_ai/01_applied_gen_ai/02_large_language_models/04_trainings/02_fine_tuning_of_transformers"
stack: "GenAI"
date: "2025-06-03T07:26:45.889Z"
draft: false
---

## Fine-Tuning

> After finetuning the model could do summarization, translation, dialogue systems, perform reasoning tasks, perform code generation.

Uptil now what we have learnt

1. `Transformers`
   - can leverage benfits of both Encoder & Decoder.
2. `Encoder-only`
   - Guarantees language Understanding
3. `Decoder-only Language model`
   - Gurantees text generation without ability to understand intent
4. `Decoder-only LLM`
   - Guarantees text generation and Understanding capability which is more like transformer but it still misses Understanding the intent
5. `Fine-tuned Decoder-only LLM`
   - Guarantees Understanding language, intent and text generation

### Types of Fine-Tuning:

1. Instruction tuning
2. `RLHF` : **R**einforcement **L**earning with **H**uman **F**eedback

### Instruction Tuning

> We trained the model on a dataset of (instruction, response) to improve its ability to follow user's commands (intent)

- `Without fine-tuning`

```py:title=input_prompt
Tell me about New Delhi
```

<op>

It may generate a long, unfocussed essay on New Delhi (history, culture, topology etc)

</op>

- `With fine-tuning`

```py:title=input_prompt
input = "Tell me about New Delhi"

prompt = f"{input} in 3 bullet points"
```

<op>

- Famous for monuments like Red fort, India Gate etc
- Known for different kinds of fruits
- Capital of India

</op>

- We understand now that my model follows instructions very well and generates the output. But there is one more question ?

❓**The output generated by understanding the instruction is optimal**

- You know that because the model has text generation ability it may generate many output for one single input
- To make your response more helpful, accurate, customized and safe we perform `RLHF`

### **R**einforcement **L**earning with **H**uman **F**eedback

- In RLHF we use **reward model training strategy**.

- The model gives multiple outputs which human raters rank responses manually to the same input instruction.
- A reward model is trained on these ranked responses. So with this the model then starts learning out of all the possible responses which is the response with highest reward.
- Our model has the ability to understand instruction, understand language, generate language and optimize generation.
- Now we found out that if you can instruct the model to behave in a certain manner it can do any task for you. This gave birth to `prompt engineering`

### Memory aspect

If you want to do conversation do you think whatever ability we saw above is suffcient?

1. Understanding user input
2. Identifying the intent
3. Generating the output
4. Optimizing the output and sending it back to the user

❓**Is this sufficient to perform conversation**

- The one critical requirement of conversation is to remember the interactions of the past. Do you think they have this ability ?
