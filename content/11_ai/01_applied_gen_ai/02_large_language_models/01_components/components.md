---
title: "Components of LLMs"
slug: "11_ai/01_applied_gen_ai/02_large_language_models/01_components"
stack: "GenAI"
date: "2025-06-03T07:26:45.889Z"
draft: false
---

## `Tokenization`

- This process involves breaking down text into smaller units called tokens, which can be words, phrases, or even individual characters.

## `Embeddings`

- This embedding component maps tokens to a high-dimensional vector space, representing each token with a unique vector.

## `Attention`

- This attention mechanism lets the model concentrate on specific parts of the input text when generating output.

## `PreTraining`

- This involves pretraining LLMs on extensive text data to understand the underlying patterns and structures of human language.

## `Transfer learning`

- This component allows the model to adapt to new tasks by fine-tuning the pre-trained model on a smaller dataset.

## `Encoder and Decoder`

- This employs the Transformer framework in a large language model architecture, comprising two main parts: an encoder and a decoder.

## `Scaling`

- This necessitates significant computational resources for training and upkeep, making scaling a challenging but essential part of its architecture.
